{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to fitting and optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, two libraries and several optimization routines will be used to fit a single crystal peak.\n",
    "- `scipy.optimize.curve_fit`\n",
    "- `scipy.optimize.least_squares`\n",
    "- `lmfit`\n",
    "\n",
    "The overview is as follows:\n",
    "1. Loading and plotting data\n",
    "2. Definining and fitting a model\n",
    "3. Inspecting and interpreting output\n",
    "\n",
    "### First begin by loading the required libaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- import libaries --\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy.optimize\n",
    "import lmfit\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading data with `h5py`\n",
    "\n",
    "Load the mantid workspace with histogramed counts vs. $d$-spacing bin edges. The `values` contain the counts and `errors` contain the uncertanties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- load data ---\n",
    "\n",
    "f = h5py.File('../data/elastic.nxs', mode='r')\n",
    "\n",
    "ws = f['mantid_workspace_1/workspace/']\n",
    "d_spacing_bin_edges = ws['axis1'][()]\n",
    "\n",
    "counts  = ws['values'][()].flatten()\n",
    "errors  = ws['errors'][()].flatten()\n",
    "\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the data with `matplotlib`\n",
    "\n",
    "It is necessary to calculate the $d$-spacing bin centers from the bin edges. The number of histogram bins is one less than the number of edges.\n",
    "$$d(\\mathrm{center})=\\frac{d(\\mathrm{left})+d(\\mathrm{right})}{2}$$\n",
    "\n",
    "Matplotib can appropriately plot errorbars.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- plot data ---\n",
    "\n",
    "d_spacing = 0.5*(d_spacing_bin_edges[1:]+d_spacing_bin_edges[:-1])\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "ax.errorbar(d_spacing, counts, yerr=errors, fmt='.', label='data')\n",
    "ax.legend(shadow=True)\n",
    "ax.minorticks_on()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining a model as a function and guessing initial parameters\n",
    "\n",
    "The peak could be modeled as a Gaussian peak with a linear sloping background.\n",
    "\n",
    "$$y=Ae^{-\\frac{(d-\\mu)^2}{2\\sigma^2}}+B+cd$$\n",
    "\n",
    "Guassian peak\n",
    "1. $A=\\mathrm{amplutude}$\n",
    "2. $\\mu=\\mathrm{mean}$\n",
    "3. $\\sigma=\\mathrm{standard\\;deviation}$\n",
    "\n",
    "Sloping background\n",
    "1. $B=\\mathrm{intercept}$\n",
    "2. $c=\\mathrm{slope}$\n",
    "\n",
    "It is illustrative to guess the initial parameters. This is often a helpful staring point for fitting a model. This initial guess is labeled as `p0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- define model ---\n",
    "\n",
    "def model(d, A, mu, sigma, B, c):\n",
    "\n",
    "    return A*np.exp(-0.5*(d-mu)**2/sigma**2)+B+c*d\n",
    "\n",
    "A, mu, sigma, B, c = 800, 16, 0.3, 100, 0\n",
    "\n",
    "p0 = (A, mu, sigma, B, c)\n",
    "\n",
    "ax.plot(d_spacing, model(d_spacing, *p0), label='guess')\n",
    "ax.legend(shadow=True)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model using `scipy.optimize.curve_fit`\n",
    "\n",
    "Since the model can be expressed as a $y=f(d,\\mathrm{parameters})$, `curve_fit` is a good candidate. Providing the model function, initial guess, independent and dependent variables, it gives optimized parameters `popt` and the covariance matrix of the parameters `pcov`.\n",
    "\n",
    "Since the experimental uncertainties are available and correctly scaled, they can be incorporated into the fit. Here, Levenberg-Marquardt method is used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- fit model ---\n",
    "\n",
    "popt, pcov = scipy.optimize.curve_fit(model, d_spacing, counts, p0=p0,\n",
    "                                      sigma=errors, absolute_sigma=True,\n",
    "                                      method='lm')\n",
    "\n",
    "ax.plot(d_spacing, model(d_spacing, *popt), label='fit')\n",
    "ax.legend(shadow=True)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the residuals\n",
    "\n",
    "It is good practice to plot the residuals of the final fit to ensure the fit is of good quality. This is indicated by absence of systematic trends whic suggest the model is not correct. If the model describes the data well, then the residuals should be fluctiations about zero corresponding to experimental uncertainty."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- plot residuals ---\n",
    "\n",
    "def residual(d, counts, A, mu, sigma, B, c):\n",
    "    \n",
    "    return counts-model(d, A, mu, sigma, B, c)\n",
    "\n",
    "ax.plot(d_spacing, residual(d_spacing, counts, *popt), label='residual')\n",
    "ax.legend(shadow=True)\n",
    "fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating errors on fitted paramters\n",
    "\n",
    "The covariance matrix provides the correlation between fitted parameters. \n",
    "\n",
    "$$c_{ij}=\\begin{pmatrix} \n",
    "c_{AA} & c_{A\\mu} & c_{A\\sigma} & c_{AB} & c_{Ac}  \\\\ \n",
    "c_{\\mu A} & c_{\\mu\\mu} & c_{\\mu\\sigma} & c_{\\mu B} & c_{\\mu c}  \\\\ \n",
    "c_{\\sigma A} & c_{\\sigma\\mu} & c_{\\sigma\\sigma} & c_{\\sigma B} & c_{\\sigma c}  \\\\ \n",
    "c_{BA} & c_{B\\mu} & c_{B\\sigma} & c_{BB} & c_{Bc}  \\\\ \n",
    "c_{cA} & c_{c\\mu} & c_{c\\sigma} & c_{cB} & c_{cc}  \\\\ \n",
    "\\end{pmatrix}$$\n",
    "\n",
    "The diagonal of the matrix corresponds to the variance of the individual parameters. Simply taking the square root gives the errors (standard deviation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- calculate errors ---\n",
    "\n",
    "perr = np.sqrt(np.diag(pcov))\n",
    "\n",
    "print('Fitted uncertanties as 1-std using curve_fit')\n",
    "print('A     = {:8.3f} ± {:5.3f}'.format(popt[0],perr[0]))\n",
    "print('mu    = {:8.3f} ± {:5.3f}'.format(popt[1],perr[1]))\n",
    "print('sigma = {:8.3f} ± {:5.3f}'.format(popt[2],perr[2]))\n",
    "print('B     = {:8.3f} ± {:5.3f}'.format(popt[3],perr[3]))\n",
    "print('c     = {:8.3f} ± {:5.3f}'.format(popt[4],perr[4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting the model using `scipy.optimize.least_squares`\n",
    "\n",
    "It is also possible to define the least squares problem equivalently to the curve fitting probem. The objective function to optimize is the $\\chi^2$ parameter which accounts for the residuals weighted by the experimental uncertainties. That is,\n",
    "\n",
    "$$\\chi^2=\\sum_i\\frac{[y_i-f(d,\\mathrm{parameters}]^2}{\\sigma_i^2}$$\n",
    "\n",
    "which is the sum of weighted deviations $[y_i-f(d,\\mathrm{parameters})]/\\sigma_i$. Levenberg-Marquardt can be used again and a solution os returned that contains the fitted parameters `sol.x` and weighted Jacobian `sol.jac`. The covariance matrix is constructed from the Jacobian according to \n",
    "\n",
    "$$\\boldsymbol{C}=\\boldsymbol{J}_w^\\intercal\\boldsymbol{J}_w$$\n",
    "\n",
    "that contains the weight matrix $\\boldsymbol{W}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- define weighted least squares problem ---\n",
    "\n",
    "def weighted_deviations(x, d, counts, errors):\n",
    "\n",
    "    A, mu, sigma, B, c = x\n",
    "\n",
    "    return residual(d, counts, A, mu, sigma, B, c)/errors\n",
    "\n",
    "sol = scipy.optimize.least_squares(weighted_deviations, x0=p0,\n",
    "                                   args=(d_spacing, counts, errors),\n",
    "                                   method='lm')\n",
    "\n",
    "vals = sol.x\n",
    "\n",
    "J = sol.jac\n",
    "cov = np.linalg.inv(J.T.dot(J))\n",
    "\n",
    "err = np.sqrt(np.diag(cov))\n",
    "\n",
    "print('Fitted uncertanties as 1-std using least_squares')\n",
    "print('A     = {:8.3f} ± {:5.3f}'.format(vals[0],err[0]))\n",
    "print('mu    = {:8.3f} ± {:5.3f}'.format(vals[1],err[1]))\n",
    "print('sigma = {:8.3f} ± {:5.3f}'.format(vals[2],err[2]))\n",
    "print('B     = {:8.3f} ± {:5.3f}'.format(vals[3],err[3]))\n",
    "print('c     = {:8.3f} ± {:5.3f}'.format(vals[4],err[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chi2dof = np.sum(sol.fun**2)/(sol.fun.size-sol.x.size)\n",
    "cov *= chi2dof\n",
    "\n",
    "stderr = np.sqrt(np.diag(cov))\n",
    "\n",
    "print('Fitted uncertanties as 1-stderr using least_squares')\n",
    "print('A     = {:8.3f} ± {:5.3f}'.format(vals[0],stderr[0]))\n",
    "print('mu    = {:8.3f} ± {:5.3f}'.format(vals[1],stderr[1]))\n",
    "print('sigma = {:8.3f} ± {:5.3f}'.format(vals[2],stderr[2]))\n",
    "print('B     = {:8.3f} ± {:5.3f}'.format(vals[3],stderr[3]))\n",
    "print('c     = {:8.3f} ± {:5.3f}'.format(vals[4],stderr[4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- use constrained optimization ---\n",
    "\n",
    "def weighted_residual(params, d, counts, errors):\n",
    "\n",
    "    A = params['A']\n",
    "    mu = params['mu']\n",
    "    sigma = params['sigma']\n",
    "    B = params['B']\n",
    "    c = params['c']\n",
    "\n",
    "    return residual(d, counts, A, mu, sigma, B, c)/errors\n",
    "\n",
    "params = lmfit.Parameters()\n",
    "params.add('A', value=A, min=0, max=np.inf)\n",
    "params.add('mu', value=mu, min=-np.inf, max=np.inf)\n",
    "params.add('sigma', value=sigma, min=0, max=np.inf)\n",
    "params.add('B', value=B, min=0, max=np.inf)\n",
    "params.add('c', value=c, min=-np.inf, max=np.inf)\n",
    "\n",
    "result = lmfit.minimize(weighted_residual, params,\n",
    "                        args=(d_spacing, counts, errors))\n",
    "\n",
    "print('Fitted uncertanties as 1-stderr using lmfit')\n",
    "for key in params.keys():\n",
    "    value, stderr = result.params[key].value, result.params[key].stderr\n",
    "    print('{:7} = {:8.3f} ± {:5.3f}'.format(key,value,stderr))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
